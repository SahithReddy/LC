/* QUESTION: 706. Design HashMap
Design a HashMap without using any built-in hash table libraries.

To be specific, your design should include these functions:

put(key, value) : Insert a (key, value) pair into the HashMap. If the value already exists in the HashMap, update the value.
get(key): Returns the value to which the specified key is mapped, or -1 if this map contains no mapping for the key.
remove(key) : Remove the mapping for the value key if this map contains the mapping for the key.

Example:
MyHashMap hashMap = new MyHashMap();
hashMap.put(1, 1);          
hashMap.put(2, 2);         
hashMap.get(1);            // returns 1
hashMap.get(3);            // returns -1 (not found)
hashMap.put(2, 1);          // update the existing value
hashMap.get(2);            // returns 1 
hashMap.remove(2);          // remove the mapping for 2
hashMap.get(2);            // returns -1 (not found) 
*/
----------------------------------------------
// https://leetcode.com/problems/design-hashmap/discuss/227081/Java-Solutions . -- See this solution directly
// Explanation: https://leetcode.com/problems/design-hashmap/solution/


/* QUESTIONS TO ASK BEFORE IMPLEMENTING:
SHOULD I ASSUME: 
All keys and values will be in the range of [0, 1000000].
The number of operations will be in the range of [1, 10000].

Some of the questions which can be asked to the interviewer before implementing the solution

*For simplicity, are the keys integers only?
*For collision resolution, can we use chaining?
*Do we have to worry about load factors? (If Yes, refer solution 2nd solution)
Can we assume inputs are valid or do we have to validate them?
Can we assume this fits memory?
*/
----------------------------------------------
/* THEORY:
The most distinguish characteristic about hashmap is that it provides a fast access to a value that is associated with a given key.

There are two main issues that we should tackle, in order to design an efficient hashmap data structure: 1). hash function design and 2). collision handling.

1) hash function design: the purpose of hash function is to map a key value to an address in the storage space, 
similarly to the system that we assign a postcode to each mail address. As one can image, for a good hash function, 
it should map different keys evenly across the storage space, so that we don't end up with the case that the 
majority of the keys are concentrated in a few spaces.
2) collision handling: essentially the hash function reduces the vast key space into a limited address space. 
As a result, there could be the case where two different keys are mapped to the same address, which is what 
we call 'collision'. Since the collision is inevitable, it is important that we have a strategy to handle the collision.

Depending on how we deal with each of the above two issues, we could have various implementation of hashmap data structure.
*/
----------------------------------------------
/* ALGORITHM:
For each of the methods in hashmap data structure, namely get(), put() and remove(), 
it all boils down to the method to locate the value that is stored in hashmap, given the key.

This localization process can be done in two steps:

For a given key value, first we apply the hash function to generate a hash key, which corresponds to the address in our main storage. With this hash key, we would find the bucket where the value should be stored.
Now that we found the bucket, we simply iterate through the bucket to check if the desired <key, value> pair does exist.
*/
----------------------------------------------
/* IMPLEMENTATION:  
Below is Hash-Table Implementation - Using Array of LinkedList
1. The general implementation of HashMap uses bucket which is basically a chain of linked lists and each node containing <key, value> pair.
2. So if we have duplicate nodes, that doesn't matter - it will still replicate each key with it's value in linked list node.
3. When we insert the pair (10, 20) and then (10, 30), there is technically no collision involved. We are just replacing 
the old value with the new value for a given key 10, since in both cases, 10 is equal to 10 and also the hash code for 
10 is always 10.
4. Collision happens when multiple keys hash to the same bucket. In that case, we need to make sure that we can 
distinguish between those keys. Chaining collision resolution is one of those techniques which is used for this.
*/
----------------------------------------------
// CODE:
class MyHashMap
{
	private static class ListNode
	{
		int key, val;
		ListNode next;

		ListNode(int key, int val)
		{
			this.key = key;
			this.val = val;
		}
	}
	ListNode[] nodes = new ListNode[10000];
	
	// ORDER: getIndex --> findElement --> put --> get --> remove
	
	/** value will always be non-negative. */
	public void put(int key, int value)
	{
		int index = getIndex(key);
        ListNode prev = findElement(index, key);
        if(prev.next == null){
            prev.next = new ListNode(key, value);
        }
        prev.next.val = value;
	}
	
	/** Returns the value to which the specified key is mapped, or -1 if this map contains no mapping for the key */
	public int get(int key)
	{
		int index = getIndex(key);
        ListNode prev = findElement(index, key);
        return prev.next==null? -1: prev.next.val;
	}
	
	/** Removes the mapping of the specified value key if this map contains a mapping for the key */
	public void remove(int key)
	{
		int index = getIndex(key);
        ListNode prev = findElement(index, key);
        if(prev.next!=null){
            prev.next = prev.next.next;
        }
	}
	
	/* Generating the hashcode */
	private int getIndex(int key){
        return Integer.hashCode(key)% nodes.length;
    	}
	
	/* Find element with that hash*/
	private ListNode findElement(int index, int key)
	{
		if(nodes[index]==null){
            return nodes[index] = new ListNode(-1, -1);
        }
        
        ListNode prev = nodes[index];
        
        while(prev.next!=null && prev.next.key!=key){
            prev = prev.next;
        }
        return prev;
	}
}

/*Time complexity: O(1) average and O(n) worst case - for all get(),put() and remove() methods.
Space complexity: O(n) - where n is the number of entries in HashMap
*/
--------------------------------------------
/* IMPROVEMENT:
Although upper bound of unique keys is given (10,000) so we can initialize nodes array with length 10000, adding rehashing can still be useful in case of followup question like:

What if most keys are redundant?

Reference: https://www.geeksforgeeks.org/load-factor-and-rehashing/

HOW HASHING WORKS:
For insertion of a key(K) – value(V) pair into a hash map, 2 steps are required:

K is converted into a small integer (called its hash code) using a hash function.
The hash code is used to find an index (hashCode % arrSize) and the entire linked list at that index(Separate chaining) is first searched for the presence of the K already.
If found, it’s value is updated and if not, the K-V pair is stored as a new node in the list.


COMPLEXITY AND LOAD FACTOR:
For the first step, time taken depends on the K and the hash function.
For example, if the key is a string “abcd”, then it’s hash function may depend on the length of the string. But for very large values of n, the number of entries into the map, length of the keys is almost negligible in comparison to n so hash computation can be considered to take place in constant time, i.e, O(1).
For the second step, traversal of the list of K-V pairs present at that index needs to be done. For this, the worst case may be that all the n entries are at the same index. So, time complexity would be O(n). But, enough research has been done to make hash functions uniformly distribute the keys in the array so this almost never happens.
So, on an average, if there are n entries and b is the size of the array there would be n/b entries on each index. This value n/b is called the load factor that represents the load that is there on our map.
This Load Factor needs to be kept low, so that number of entries at one index is less and so is the complexity almost constant, i.e., O(1).


REHASHING:
As the name suggests, rehashing means hashing again. Basically, when the load factor increases to more than its pre-defined value (default value of load factor is 0.75), the complexity increases. So to overcome this, the size of the array is increased (doubled) and all the values are hashed again and stored in the new double sized array to maintain a low load factor and low complexity.

WHY REHASHING?
Rehashing is done because whenever key value pairs are inserted into the map, the load factor increases, which implies that the time complexity also increases as explained above. This might not give the required time complexity of O(1).

Hence, rehash must be done, increasing the size of the bucketArray so as to reduce the load factor and the time complexity.


HOW REHASHING IS DONE:
Rehashing can be done as follows:

For each addition of a new entry to the map, check the load factor.
If it’s greater than its pre-defined value (or default value of 0.75 if not given), then Rehash.
For Rehash, make a new array of double the previous size and make it the new bucketarray.
Then traverse to each element in the old bucketArray and call the insert() for each so as to insert it into the new larger bucket array.
*/
----------------------------------------------
class MyHashMap {
    private static final double LOAD_FACTOR = 0.75;
    private Node[] nodes;
    private int size; // number of keys

    /** Initialize your data structure here. */
    public MyHashMap() {
        nodes = new Node[5];
        size = 0;
    }
    
    /** value will always be non-negative. */
    public void put(int key, int value) {
        int idx = hash(key);
        for (Node x = nodes[idx]; x != null; x = x.next) {
            if (x.key == key) {
                x.value = value;
                return;
            }
        }
        nodes[idx] = new Node(key, value, nodes[idx]);
        size++;
        
        double loadFactor = (double) size / nodes.length;
        if (loadFactor > LOAD_FACTOR) {
            rehash();
        }
    }
    
    /** Returns the value to which the specified key is mapped, or -1 if this map contains no mapping for the key */
    public int get(int key) {
        int idx = hash(key);
        for (Node x = nodes[idx]; x != null; x = x.next) {
            if (x.key == key) {
                return x.value;
            }
        }
        return -1;
    }
    
    /** Removes the mapping of the specified value key if this map contains a mapping for the key */
    public void remove(int key) {
        int idx = hash(key);
        Node pre = new Node(-1, -1, nodes[idx]); // sentinal node before list head
        for (Node prev = pre; prev.next != null; prev = prev.next) {
            if (prev.next.key == key) {
                prev.next = prev.next.next;
                break;
            }
        }
        nodes[idx] = pre.next;
		size--;
    }
    
    private int hash(int key) {
        return key % nodes.length;
    }
    
    private void rehash() {
        Node[] tmp = nodes;
        nodes = new Node[tmp.length * 2];
        size = 0;
        for (Node head: tmp) {
            for (Node x = head; x != null; x = x.next) {
                put(x.key, x.value);
            }
        }
    }
    
    class Node {
        int key;
        int value;
        Node next;

        public Node(int key, int value, Node next) {
            this.key = key;
            this.value = value;
            this.next = next;
        }
    }
}
